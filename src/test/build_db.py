import os, re, json, faiss, numpy as np, gc
from tqdm.auto import tqdm
from sentence_transformers import SentenceTransformer
from langchain_text_splitters import RecursiveCharacterTextSplitter
from src.configs.config import RAGIndexArgs
from src.utils.print_utils import printi
from src.data.record import Metadata

# ko-ww-stopwords ë¼ì´ë¸ŒëŸ¬ë¦¬ import
try:
    from ko_ww_stopwords.stop_words import ko_ww_stop_words
    from ko_ww_stopwords.tools import is_stop_word
    STOPWORDS_AVAILABLE = True
    printi("âœ… ko-ww-stopwords ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì„±ê³µ")
except ImportError:
    STOPWORDS_AVAILABLE = False
    printi("âš ï¸ ko-ww-stopwords ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ. pip install ko-ww-stopwords")

# í—¤ë”ë§Œ ì¡ìŒ
hdr_pat = re.compile(r'^\s*=+\s*(.+?)\s*=+\s*$', re.MULTILINE)
bullet_pat = re.compile(r'^\s*[â€¢\-\â€“Â·\*]\s+', re.MULTILINE)
blank_pat = re.compile(r'\n{2,}')


def estimate_token_count(text: str) -> int:
    """
    í•œêµ­ì–´ í…ìŠ¤íŠ¸ì˜ ëŒ€ëµì ì¸ í† í° ìˆ˜ ì¶”ì •
    í•œêµ­ì–´ëŠ” ëŒ€ëµ 1.2-1.5 ë¬¸ìë‹¹ 1í† í° ì •ë„
    """
    # ê³µë°± ì œê±° í›„ ë¬¸ì ìˆ˜ ê³„ì‚°
    clean_text = re.sub(r'\s+', '', text)
    # í•œêµ­ì–´ëŠ” ë¬¸ìë‹¹ ì•½ 0.8-1.0 í† í°ìœ¼ë¡œ ì¶”ì •
    estimated_tokens = len(clean_text) * 0.9
    return int(estimated_tokens)

def remove_stopwords(text: str) -> tuple[str, list[str]]:
    """
    ko-ww-stopwordsë¥¼ ì‚¬ìš©í•œ ë¶ˆìš©ì–´ ì œê±°
    Returns: (filtered_text, removed_stopwords)
    """
    if not STOPWORDS_AVAILABLE:
        return text, []  # ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìœ¼ë©´ ì›ë³¸ í…ìŠ¤íŠ¸ì™€ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

    words = text.split()
    filtered_words = []
    removed_stopwords = []

    for word in words:
        if is_stop_word(word):
            removed_stopwords.append(word)
        else:
            filtered_words.append(word)

    return ' '.join(filtered_words), removed_stopwords

def light_preprocess_text(text: str) -> tuple[str, list[str]]:
    """
    ê°€ë²¼ìš´ ì „ì²˜ë¦¬ - ë¬¸ë§¥ê³¼ ì˜ë¯¸ë¥¼ ë³´ì¡´í•˜ë©´ì„œ ë…¸ì´ì¦ˆë§Œ ì œê±°
    ì €ì¥ê³¼ ì„ë² ë”© ëª¨ë‘ ì´ í…ìŠ¤íŠ¸ ì‚¬ìš©
    Returns: (processed_text, removed_stopwords)
    """
    if not text or not text.strip():
        return "", []

    # 1. HTML íƒœê·¸, URL, ì´ë©”ì¼ ì œê±°
    text = re.sub(r'<[^>]+>', '', text)  # HTML íƒœê·¸
    text = re.sub(r'https?://[^\s]+', '', text)  # URL
    text = re.sub(r'www\.[^\s]+', '', text)  # www ë§í¬
    text = re.sub(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', '', text)  # ì´ë©”ì¼

    # 2. í•œì ì œê±° (ì¤‘êµ­ì–´ í•œì, ì¼ë³¸ì–´ í•œì, í•œêµ­ì–´ í•œì ëª¨ë‘ í¬í•¨)
    text = re.sub(r'[\u4e00-\u9fff\u3400-\u4dbf\uf900-\ufaff]', '', text)  # í•œì ì œê±°

    # 3. ê·¹ë‹¨ì ì¸ íŠ¹ìˆ˜ë¬¸ìë§Œ ì œê±° (ì˜ë¯¸ìˆëŠ” ë¬¸ì¥ë¶€í˜¸ëŠ” ìœ ì§€)
    text = re.sub(r'[^\w\sê°€-í£.,!?;:()\-\'""]', ' ', text)  # ê¸°ë³¸ ë¬¸ì¥ë¶€í˜¸ëŠ” ìœ ì§€

    # 4. ë¶ˆìš©ì–´ ì œê±° (ìƒˆë¡œ ì¶”ê°€)
    text, removed_stopwords = remove_stopwords(text)

    # 5. ì—°ì†ëœ ê³µë°±ê³¼ ì¤„ë°”ê¿ˆ ì •ë¦¬
    text = re.sub(r'\s+', ' ', text)  # ì—°ì† ê³µë°±ì„ í•˜ë‚˜ë¡œ
    text = text.strip()

    return text, removed_stopwords

def is_korean_text(text: str, korean_ratio_threshold: float = 0.5) -> bool:
    """
    í…ìŠ¤íŠ¸ê°€ í•œêµ­ì–´ì¸ì§€ íŒë‹¨ (ì„ê³„ê°’ì„ ë‚®ì¶¤)
    """
    if not text.strip():
        return False

    # ê³µë°± ì œê±°
    content_chars = text.replace(' ', '')

    if len(content_chars) < 5:
        return False

    # ìˆœìˆ˜ í•œê¸€ ë¬¸ìë§Œ ê³„ì‚°
    korean_chars = len(re.findall(r'[ê°€-í£]', content_chars))

    # í•œêµ­ì–´ ë¹„ìœ¨ ê³„ì‚°
    korean_ratio = korean_chars / len(content_chars)

    return korean_ratio >= korean_ratio_threshold

def clean(txt: str) -> str:
    """ê¸°ë³¸ í…ìŠ¤íŠ¸ ì •ë¦¬"""
    # ì¤„ ë‹¨ìœ„ë¡œ í—¤ë” ì •ì œ
    lines = txt.split('\n')
    lines = [hdr_pat.sub(r'[SECTION]\1', line) for line in lines]
    txt = '\n'.join(lines)

    # [SECTION]= = ì œëª© = = â†’ [SECTION]ì œëª©  ë¡œ ì •ë¦¬
    txt = re.sub(r'\[SECTION\][\s=]*([\wê°€-í£A-Za-z0-9 _\-()]+)[\s=]*', r'[SECTION]\1', txt)

    txt = bullet_pat.sub('', txt)
    txt = blank_pat.sub('\n', txt)
    return txt.strip()

def load_documents(path: str) -> list[tuple[str, str]]:
    """ë¬¸ì„œ ë¡œë“œ"""
    documents = []
    current_title = None
    current_body = []

    with open(path, encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue

            # level-1 ì œëª©ì„ ë¬¸ì„œ ì‹œì‘ ì‹ í˜¸ë¡œ ì‚¬ìš©
            if re.match(r'^\s*=+\s*[^=]+?\s*=+\s*$', line):
                if current_title and current_body:
                    documents.append((current_title, "\n".join(current_body).strip()))
                    current_body = []
                current_title = re.sub(r'^\s*=+\s*|\s*=+\s*$', '', line).strip()
            else:
                current_body.append(line)

    if current_title and current_body:
        documents.append((current_title, "\n".join(current_body).strip()))

    return documents

def process_batch_and_save(chunks_batch, metas_batch, model, index, meta_file_handle, batch_size):
    """ë°°ì¹˜ë¥¼ ì„ë² ë”©í•˜ê³  ë°”ë¡œ ì €ì¥"""
    if not chunks_batch:
        return

    # ì„ë² ë”© ìƒì„± (ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ë¡œ)
    vecs = model.encode(
        chunks_batch,
        batch_size=batch_size,
        show_progress_bar=True,
        normalize_embeddings=True
    )

    # FAISS ì¸ë±ìŠ¤ì— ì¶”ê°€
    index.add(np.float32(vecs))

    # ë©”íƒ€ë°ì´í„° íŒŒì¼ì— ë°”ë¡œ ì €ì¥
    for meta in metas_batch:
        meta_file_handle.write(json.dumps(meta.__dict__, ensure_ascii=False) + "\n")

    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del vecs
    gc.collect()

    printi(f"Processed batch of {len(chunks_batch)} chunks")

def process_single_corpus(corpus_info: dict, rag_index_args: RAGIndexArgs, model, splitter,
                         korean_ratio_threshold: float) -> tuple:

    MIN_TOKEN_COUNT = 32

    """ë‹¨ì¼ ì½”í¼ìŠ¤ë¥¼ ì²˜ë¦¬í•˜ê³  ê°œë³„ íŒŒì¼ë¡œ ì €ì¥"""
    c_dir = corpus_info["dir"]
    c_name = os.path.basename(c_dir)
    c_base = corpus_info["base"]
    exts = corpus_info["ext"]

    # ê°œë³„ ì½”í¼ìŠ¤ìš© ë””ë ‰í† ë¦¬ ìƒì„±
    corpus_index_dir = os.path.join(rag_index_args.index_dir, c_name)
    os.makedirs(corpus_index_dir, exist_ok=True)

    # ì²« ë²ˆì§¸ ë°°ì¹˜ë¡œ ì°¨ì› í™•ì¸ì„ ìœ„í•œ ë”ë¯¸ ì„ë² ë”©
    dummy_vec = model.encode(["dummy"], normalize_embeddings=True)
    dim = dummy_vec.shape[1]

    # ê°œë³„ FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™”
    corpus_index = faiss.IndexFlatIP(dim)

    # ê°œë³„ ë©”íƒ€ë°ì´í„° íŒŒì¼ ê²½ë¡œ
    corpus_meta_path = os.path.join(corpus_index_dir, f"{c_name}_{rag_index_args.meta_base}")
    corpus_index_path = os.path.join(corpus_index_dir, f"{c_name}_{rag_index_args.index_base}")

    chunks_batch = []
    metas_batch = []
    total_processed = 0
    total_filtered_korean = 0
    total_filtered_preprocessing = 0
    total_filtered_stopwords = 0  # ë¶ˆìš©ì–´ ì œê±° ì¹´ìš´íŠ¸ ì¶”ê°€
    total_filtered_duplicates = 0  # ì¤‘ë³µ ì œê±° ì¹´ìš´íŠ¸ ì¶”ê°€

    # ë¶ˆìš©ì–´ í†µê³„ë¥¼ ìœ„í•œ ë³€ìˆ˜ë“¤
    all_removed_stopwords = []
    stopword_examples_shown = False

    # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ ì„¸íŠ¸
    seen_texts = set()

    MEMORY_BATCH_SIZE = 10000

    printi(f"ğŸ”„ Processing corpus: {c_name}")

    with open(corpus_meta_path, "w", encoding="utf-8") as meta_file:
        for ext in exts:
            txt = f"{c_name}_{c_base}.{ext}"
            path = os.path.join(c_dir, txt)
            printi(f"Loading {path}")

            if not os.path.isfile(path):
                continue

            documents = load_documents(path)
            printi(f"Loaded {len(documents)} documents from {path}")

            for title, body in tqdm(documents, desc=f"{c_name}-{ext}", unit="docs"):
                clean_body = clean(body)
                if not clean_body:
                    continue

                doc_chunks = splitter.split_text(clean_body)

                for ch in doc_chunks:
                    # 1. í•œêµ­ì–´ í…ìŠ¤íŠ¸ ê²€ì‚¬ (ì „ì²˜ë¦¬ ì „)
                    if not is_korean_text(ch, korean_ratio_threshold):
                        total_filtered_korean += 1
                        continue

                    # 2. ê°€ë²¼ìš´ ì „ì²˜ë¦¬ + ë¶ˆìš©ì–´ ì œê±° ì ìš©
                    processed_ch, removed_stopwords = light_preprocess_text(ch)

                    # ë¶ˆìš©ì–´ í†µê³„ ìˆ˜ì§‘
                    if removed_stopwords:
                        all_removed_stopwords.extend(removed_stopwords)

                        # ì²˜ìŒ ëª‡ ê°œ ì˜ˆì‹œë§Œ ì¶œë ¥
                        if not stopword_examples_shown and len(all_removed_stopwords) >= 10:
                            unique_stopwords = list(set(all_removed_stopwords))[:20]
                            printi(f"ğŸ“‹ ì œê±°ëœ ë¶ˆìš©ì–´ ì˜ˆì‹œ (ìƒìœ„ 20ê°œ): {', '.join(unique_stopwords)}")
                            stopword_examples_shown = True

                    # 3. ì „ì²˜ë¦¬ í›„ ë¹ˆ í…ìŠ¤íŠ¸ ì²´í¬
                    if not processed_ch or len(processed_ch.strip()) < 15:
                        total_filtered_preprocessing += 1
                        continue

                    # 4. í† í° ìˆ˜ ì²´í¬
                    estimated_tokens = estimate_token_count(processed_ch)
                    if estimated_tokens < MIN_TOKEN_COUNT:
                        total_filtered_preprocessing += 1
                        continue

                    # 5. ë¶ˆìš©ì–´ ì œê±° í›„ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ì•„ì¡ŒëŠ”ì§€ ì²´í¬
                    if len(processed_ch.split()) < 3:  # ë‹¨ì–´ê°€ 3ê°œ ë¯¸ë§Œì´ë©´ ì œì™¸
                        total_filtered_stopwords += 1
                        continue

                    # 6. ì¤‘ë³µ í…ìŠ¤íŠ¸ ì²´í¬ (ìƒˆë¡œ ì¶”ê°€)
                    if processed_ch in seen_texts:
                        total_filtered_duplicates += 1
                        continue

                    # ì¤‘ë³µì´ ì•„ë‹Œ ê²½ìš° ì„¸íŠ¸ì— ì¶”ê°€
                    seen_texts.add(processed_ch)

                    # ì €ì¥ê³¼ ì„ë² ë”© ëª¨ë‘ ê°™ì€ ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ ì‚¬ìš©
                    chunks_batch.append(processed_ch)

                    # ë©”íƒ€ë°ì´í„°ì— ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ ì €ì¥
                    meta = Metadata(
                        corpus=c_name,
                        split=ext,
                        title=title,
                        text=processed_ch  # ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ ì €ì¥
                    )

                    metas_batch.append(meta)

                    # ë°°ì¹˜ í¬ê¸°ì— ë„ë‹¬í•˜ë©´ ì²˜ë¦¬
                    if len(chunks_batch) >= MEMORY_BATCH_SIZE:
                        process_batch_and_save(
                            chunks_batch, metas_batch, model, corpus_index,
                            meta_file, rag_index_args.batch_size
                        )
                        total_processed += len(chunks_batch)
                        chunks_batch = []
                        metas_batch = []

        # ë‚¨ì€ ë°°ì¹˜ ì²˜ë¦¬
        if chunks_batch:
            process_batch_and_save(
                chunks_batch, metas_batch, model, corpus_index,
                meta_file, rag_index_args.batch_size
            )
            total_processed += len(chunks_batch)

    # ê°œë³„ ì¸ë±ìŠ¤ ì €ì¥
    faiss.write_index(corpus_index, corpus_index_path)

    # ë¶ˆìš©ì–´ í†µê³„ ì¶œë ¥
    if all_removed_stopwords:
        from collections import Counter
        stopword_counter = Counter(all_removed_stopwords)
        most_common_stopwords = stopword_counter.most_common(15)
        printi(f"ğŸ“Š ê°€ì¥ ë§ì´ ì œê±°ëœ ë¶ˆìš©ì–´ (ìƒìœ„ 15ê°œ):")
        for word, count in most_common_stopwords:
            printi(f"   '{word}': {count:,}íšŒ")
        printi(f"ğŸ“ˆ ì´ ì œê±°ëœ ë¶ˆìš©ì–´ ìˆ˜: {len(all_removed_stopwords):,}ê°œ")
        printi(f"ğŸ“ˆ ê³ ìœ  ë¶ˆìš©ì–´ ì¢…ë¥˜: {len(set(all_removed_stopwords)):,}ê°œ")

    printi(f"âœ… {c_name} ì²˜ë¦¬ ì™„ë£Œ:")
    printi(f"  - ì²˜ë¦¬ëœ ì²­í¬: {total_processed:,}")
    printi(f"  - í•œêµ­ì–´ í•„í„°ë§ ì œê±°: {total_filtered_korean:,}")
    printi(f"  - ì „ì²˜ë¦¬ í•„í„°ë§ ì œê±°: {total_filtered_preprocessing:,}")
    printi(f"  - ë¶ˆìš©ì–´ ì œê±° í›„ ì œê±°: {total_filtered_stopwords:,}")
    printi(f"  - ì¤‘ë³µ í…ìŠ¤íŠ¸ ì œê±°: {total_filtered_duplicates:,}")
    printi(f"  - ê³ ìœ  í…ìŠ¤íŠ¸ ìˆ˜: {len(seen_texts):,}")
    printi(f"  - ì¸ë±ìŠ¤ ì €ì¥: {corpus_index_path}")
    printi(f"  - ë©”íƒ€ë°ì´í„° ì €ì¥: {corpus_meta_path}")

    return total_processed, total_filtered_korean, total_filtered_preprocessing + total_filtered_stopwords + total_filtered_duplicates

def merge_indices(rag_index_args: RAGIndexArgs, corpus_names: list[str]) -> None:
    """ê°œë³„ ì¸ë±ìŠ¤ë“¤ì„ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°"""
    printi("ğŸ”— ì¸ë±ìŠ¤ ë³‘í•© ì‹œì‘...")

    # ì²« ë²ˆì§¸ ì¸ë±ìŠ¤ë¡œ ì°¨ì› í™•ì¸
    first_corpus = corpus_names[0]
    first_index_path = os.path.join(rag_index_args.index_dir, first_corpus, f"{first_corpus}_{rag_index_args.index_base}")
    first_index = faiss.read_index(first_index_path)
    dim = first_index.d

    # í†µí•© ì¸ë±ìŠ¤ ìƒì„±
    merged_index = faiss.IndexFlatIP(dim)
    merged_meta_path = os.path.join(rag_index_args.index_dir, rag_index_args.meta_base)
    merged_index_path = os.path.join(rag_index_args.index_dir, rag_index_args.index_base)

    total_vectors = 0

    with open(merged_meta_path, "w", encoding="utf-8") as merged_meta_file:
        for corpus_name in corpus_names:
            # ê°œë³„ ì¸ë±ìŠ¤ ë¡œë“œ
            corpus_index_path = os.path.join(rag_index_args.index_dir, corpus_name, f"{corpus_name}_{rag_index_args.index_base}")
            corpus_meta_path = os.path.join(rag_index_args.index_dir, corpus_name, f"{corpus_name}_{rag_index_args.meta_base}")

            if not os.path.exists(corpus_index_path):
                printi(f"âš ï¸ {corpus_index_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                continue

            corpus_index = faiss.read_index(corpus_index_path)

            # ë²¡í„° ë³µì‚¬
            vectors = np.zeros((corpus_index.ntotal, dim), dtype=np.float32)
            corpus_index.reconstruct_n(0, corpus_index.ntotal, vectors)
            merged_index.add(vectors)

            # ë©”íƒ€ë°ì´í„° ë³µì‚¬
            with open(corpus_meta_path, "r", encoding="utf-8") as corpus_meta_file:
                for line in corpus_meta_file:
                    merged_meta_file.write(line)

            total_vectors += corpus_index.ntotal
            printi(f"  - {corpus_name}: {corpus_index.ntotal:,} vectors ë³‘í•©")

    # í†µí•© ì¸ë±ìŠ¤ ì €ì¥
    faiss.write_index(merged_index, merged_index_path)

    printi(f"âœ… ì¸ë±ìŠ¤ ë³‘í•© ì™„ë£Œ:")
    printi(f"  - ì´ ë²¡í„° ìˆ˜: {total_vectors:,}")
    printi(f"  - í†µí•© ì¸ë±ìŠ¤: {merged_index_path}")
    printi(f"  - í†µí•© ë©”íƒ€ë°ì´í„°: {merged_meta_path}")

def main(rag_index_args: RAGIndexArgs, process_separately: bool = True):
    """
    ë©”ì¸ í•¨ìˆ˜ - ê°€ë²¼ìš´ ì „ì²˜ë¦¬ë¡œ ë¬¸ë§¥ ë³´ì¡´ + ë¶ˆìš©ì–´ ì œê±° + ì¤‘ë³µ ì œê±°
    """
    # ë°°ì¹˜ í¬ê¸° ì„¤ì • (ë©”ëª¨ë¦¬ì— ë§ê²Œ ì¡°ì •)
    KOREAN_RATIO_THRESHOLD = 0.5  # í•œêµ­ì–´ ë¹„ìœ¨ ì„ê³„ê°’ì„ ë‚®ì¶¤ (50%)

    os.makedirs(rag_index_args.index_dir, exist_ok=True)

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=rag_index_args.chunk_size,
        chunk_overlap=rag_index_args.chunk_overlap
    )

    # ëª¨ë¸ ë¯¸ë¦¬ ë¡œë“œ
    printi("Loading embedding model...")
    model = SentenceTransformer(
        rag_index_args.model_name,
        device="cuda"
    )

    printi(f"ğŸ”§ ì „ì²˜ë¦¬ ì„¤ì •:")
    printi(f"   - ê°€ë²¼ìš´ ì „ì²˜ë¦¬: HTML/URL ì œê±°, ê·¹ë‹¨ì  íŠ¹ìˆ˜ë¬¸ìë§Œ ì œê±°")
    printi(f"   - í•œì ì œê±°: ëª¨ë“  í•œì ë¬¸ì ì œê±°")
    printi(f"   - ë¶ˆìš©ì–´ ì œê±°: ko-ww-stopwords {'ì‚¬ìš©' if STOPWORDS_AVAILABLE else 'ë¯¸ì‚¬ìš©'}")
    printi(f"   - ì¤‘ë³µ ì œê±°: ë™ì¼í•œ í…ìŠ¤íŠ¸ ì²­í¬ ì œê±°")
    printi(f"   - ë¬¸ë§¥ ë³´ì¡´: ë¬¸ì¥ë¶€í˜¸, ì¡°ì‚¬, ì–´ë¯¸ ìœ ì§€")
    printi(f"   - í•œêµ­ì–´ ë¹„ìœ¨ ì„ê³„ê°’: {KOREAN_RATIO_THRESHOLD*100}%")
    printi(f"   - ê°œë³„ ì²˜ë¦¬: {'ì˜ˆ' if process_separately else 'ì•„ë‹ˆì˜¤'}")

    if process_separately:
        # ê°œë³„ ì²˜ë¦¬ ëª¨ë“œ
        total_processed_all = 0
        total_filtered_korean_all = 0
        total_filtered_preprocessing_all = 0
        corpus_names = []

        for corpus_info in rag_index_args.raw_text_dir:
            corpus_name = os.path.basename(corpus_info["dir"])
            corpus_names.append(corpus_name)

            processed, filtered_korean, filtered_preprocessing = process_single_corpus(
                corpus_info, rag_index_args, model, splitter, KOREAN_RATIO_THRESHOLD
            )

            total_processed_all += processed
            total_filtered_korean_all += filtered_korean
            total_filtered_preprocessing_all += filtered_preprocessing

        # ê°œë³„ ì¸ë±ìŠ¤ë“¤ì„ í•˜ë‚˜ë¡œ ë³‘í•©
        merge_indices(rag_index_args, corpus_names)

        total_input = total_processed_all + total_filtered_korean_all + total_filtered_preprocessing_all

        printi(f"\nğŸ“Š ì „ì²´ ìµœì¢… ê²°ê³¼:")
        printi(f"âœ… ì´ ì…ë ¥ ì²­í¬: {total_input:,}")
        printi(f"âœ… ì²˜ë¦¬ëœ ì²­í¬: {total_processed_all:,} ({total_processed_all/total_input*100:.1f}%)")
        printi(f"ğŸš« í•œêµ­ì–´ í•„í„°ë§ ì œê±°: {total_filtered_korean_all:,} ({total_filtered_korean_all/total_input*100:.1f}%)")
        printi(f"ğŸš« ì „ì²˜ë¦¬ í•„í„°ë§ ì œê±°: {total_filtered_preprocessing_all:,} ({total_filtered_preprocessing_all/total_input*100:.1f}%)")

    else:
        # ê¸°ì¡´ í†µí•© ì²˜ë¦¬ ëª¨ë“œ (ì´ì „ ì½”ë“œì™€ ë™ì¼)
        printi("âš ï¸ í†µí•© ì²˜ë¦¬ ëª¨ë“œëŠ” ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    rag_index_args = RAGIndexArgs()
    # process_separately=Trueë¡œ ì„¤ì •í•˜ë©´ ê°œë³„ ì²˜ë¦¬ í›„ ë³‘í•©
    main(rag_index_args=rag_index_args, process_separately=True)
