import os, json, shutil, random

def load_json(p):
    with open(p, encoding="utf-8") as f: return json.load(f)

def save_json(data, p):
    with open(p, "w", encoding="utf-8") as f: json.dump(data, f, ensure_ascii=False, indent=2)

SHUFFLE = True  # Trueë©´ other ë°ì´í„°ë“¤ë¼ë¦¬ë§Œ ëœë¤ ì…”í”Œ


"""
1:
"datasets/CLIcK_converted",


2:
"datasets/KoAlpaca-v1.1a_converted",


3:
"datasets/etri_qa_a_converted",
"datasets/etri_qa_b_converted",
"datasets/etri_qa_c_converted",


4:
"datasets/etri_mrc_converted",


5:
"datasets/KorWikiTQ_ko_converted",

6:
"datasets/KMMLU_converted",


7:
"datasets/tydiqa-goldp_converted",


8:
"datasets/squad_kor_v1_converted",

"""

"""
1-3-cot:
datasets/CLIcK_converted_cot_refined_4.1_converted


"""



if __name__ == "__main__":
    data_number = "1-3-cot"
    original_dataset = "datasets/sub_3_data_korean_culture_qa_V1.0_preprocessed_cot_refined_4.1_converted"
    other_datasets = [
        "datasets/CLIcK_converted_cot_refined_4.1_converted",
        "datasets/etri_qa_abc_cot_refined_4.1_converted"
        # "datasets/CLIcK_converted_cot_refined_4.1_converted"
        # "datasets/CLIcK_converted",
        # "datasets/KoAlpaca-v1.1a_converted",
        # "datasets/etri_qa_a_converted",
        # "datasets/etri_qa_b_converted",
        # "datasets/etri_qa_c_converted",
        # "datasets/etri_mrc_converted",
        # "datasets/KorWikiTQ_ko_converted",
        # "datasets/KMMLU_converted",
        # "datasets/tydiqa-goldp_converted",
        # "datasets/squad_kor_v1_converted",

    ]

    target_dataset = f"datasets/merged_dataset_no_aug_v{data_number}"
    os.makedirs(target_dataset, exist_ok=True)

    # â‘  test.jsonì€ ì›ë³¸ ê·¸ëŒ€ë¡œ ë³µì‚¬
    shutil.copy2(os.path.join(original_dataset, "test.json"),
                 os.path.join(target_dataset, "test.json"))
    print(f"Copied test.json from {original_dataset} to {target_dataset}")

    # â‘¡ train.json, dev.json ë“± ë‚˜ë¨¸ì§€ ê°œë³„ íŒŒì¼ ë³‘í•©
    for fname in ["train.json", "dev.json"]:
        merged, next_id = [], 0
        original_count = 0

        print(f"\n=== Processing {fname} ===")

        # 1) ì›ë³¸ ë°ì´í„° ë¨¼ì € ì¶”ê°€
        original_path = os.path.join(original_dataset, fname)
        if os.path.exists(original_path):
            original_data = load_json(original_path)
            original_count = len(original_data)
            for item in original_data:
                item["id"] = str(next_id)
                next_id += 1
                merged.append(item)
            print(f"ğŸ“ ì›ë³¸ ë°ì´í„°: {original_count:,}ê°œ ìƒ˜í”Œ")
        else:
            print(f"ğŸ“ ì›ë³¸ ë°ì´í„°: 0ê°œ ìƒ˜í”Œ (íŒŒì¼ ì—†ìŒ)")

        # 2) other ë°ì´í„°ë“¤ ìˆ˜ì§‘ í›„ ì„ê¸°
        other_data = []
        total_other_count = 0
        print(f"ğŸ“ ì¶”ê°€ ë°ì´í„°:")
        for d in other_datasets:
            src_path = os.path.join(d, fname)
            if os.path.exists(src_path):
                dataset_items = load_json(src_path)
                dataset_count = len(dataset_items)
                total_other_count += dataset_count
                for item in dataset_items:
                    item["id"] = str(next_id)
                    next_id += 1
                    other_data.append(item)
                print(f"   - {os.path.basename(d)}: {dataset_count:,}ê°œ")
            else:
                print(f"   - {os.path.basename(d)}: 0ê°œ (íŒŒì¼ ì—†ìŒ)")

        # 3) other ë°ì´í„°ë“¤ë¼ë¦¬ë§Œ ì…”í”Œ
        if SHUFFLE and other_data:
            random.shuffle(other_data)
            print(f"ğŸ”€ ì¶”ê°€ ë°ì´í„° ì…”í”Œ ì™„ë£Œ: {total_other_count:,}ê°œ")

        # 4) ì›ë³¸ + other(ì…”í”Œëœ) ìˆœì„œë¡œ ë³‘í•©
        merged.extend(other_data)

        save_json(merged, os.path.join(target_dataset, fname))

        print(f"ğŸ’¾ ìµœì¢… ë³‘í•© ê²°ê³¼:")
        print(f"   - ì›ë³¸ ë°ì´í„°: {original_count:,}ê°œ")
        print(f"   - ì¶”ê°€ ë°ì´í„°: {total_other_count:,}ê°œ")
        print(f"   - ì „ì²´ ìƒ˜í”Œ: {len(merged):,}ê°œ")
        print(f"   - ì €ì¥ ê²½ë¡œ: {target_dataset}/{fname}")
        print("-" * 50)
