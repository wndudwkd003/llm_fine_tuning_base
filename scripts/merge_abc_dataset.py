import os, json, shutil, random

def load_json(p):
    with open(p, encoding="utf-8") as f: return json.load(f)

def save_json(data, p):
    with open(p, "w", encoding="utf-8") as f: json.dump(data, f, ensure_ascii=False, indent=2)

SHUFFLE = True  # Trueë©´ ë°ì´í„°ë“¤ë¼ë¦¬ ëœë¤ ì…”í”Œ

if __name__ == "__main__":
    # etri_qa_a, b, c ë°ì´í„°ì…‹ë§Œ ë³‘í•©
    datasets = [
        "datasets/etri_qa_a_converted",
        "datasets/etri_qa_b_converted",
        "datasets/etri_qa_c_converted",
    ]

    target_dataset = "datasets/etri_qa_abc"
    os.makedirs(target_dataset, exist_ok=True)

    # train.json, dev.json, test.json íŒŒì¼ë“¤ì„ ë³‘í•©
    for fname in ["train.json", "dev.json"]:
        merged, next_id = [], 0
        total_count = 0

        print(f"\n=== Processing {fname} ===")

        # ê° ë°ì´í„°ì…‹ì—ì„œ ë°ì´í„° ìˆ˜ì§‘
        all_data = []
        print(f"ğŸ“ ë°ì´í„° ìˆ˜ì§‘:")
        for d in datasets:
            src_path = os.path.join(d, fname)
            if os.path.exists(src_path):
                dataset_items = load_json(src_path)
                dataset_count = len(dataset_items)
                total_count += dataset_count
                for item in dataset_items:
                    item["id"] = str(next_id)
                    next_id += 1
                    all_data.append(item)
                print(f"   - {os.path.basename(d)}: {dataset_count:,}ê°œ")
            else:
                print(f"   - {os.path.basename(d)}: 0ê°œ (íŒŒì¼ ì—†ìŒ)")

        # ë°ì´í„° ì…”í”Œ
        if SHUFFLE and all_data:
            random.shuffle(all_data)
            print(f"ğŸ”€ ë°ì´í„° ì…”í”Œ ì™„ë£Œ: {total_count:,}ê°œ")

        # ë³‘í•©ëœ ë°ì´í„° ì €ì¥
        merged = all_data
        save_json(merged, os.path.join(target_dataset, fname))

        print(f"ğŸ’¾ ìµœì¢… ë³‘í•© ê²°ê³¼:")
        print(f"   - ì „ì²´ ìƒ˜í”Œ: {len(merged):,}ê°œ")
        print(f"   - ì €ì¥ ê²½ë¡œ: {target_dataset}/{fname}")
        print("-" * 50)
